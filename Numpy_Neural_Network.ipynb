{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7f769c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Neural network in numpy\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399256ba",
   "metadata": {},
   "source": [
    "## XOR Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6e7da643",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y_train = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "181118f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN:input(2) --> Hidden Layer DC (3 neurons)-->Activation Layer (3 neurons)---> Final Layer (1 neuron)--->Activation Layer(1 neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cf1607ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid Function\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "#Derivative of Sigmoid Function\n",
    "def derivatives_sigmoid(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c3dbc099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x);\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5af4e867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2));\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "46811263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9ea6ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable initialization\n",
    "\n",
    "epoch=50000 #Setting training iterations\n",
    "lr=0.1 #Setting learning rate\n",
    "inputlayer_neurons = x_train.shape[1] #number of features in data set\n",
    "hiddenlayer_neurons = 3 #number of hidden layers neurons\n",
    "output_neurons = 1 #number of neurons at output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "88b8dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight and bias initialization\n",
    "wh1=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))  # number of weights between two layers = i x j ||\n",
    "bh1=np.random.uniform(size=(1,hiddenlayer_neurons))  ## number of biases = number of output neurons / neurons in next layer\n",
    "\n",
    "wout1=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
    "bout1=np.random.uniform(size=(1,output_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "145f3184",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epoch):\n",
    "    \n",
    "    hiddenlayer_values = np.dot(x_train, wh1)+bh1  #hidden_layer_input\n",
    "    #activationlayer_1 = tanh(hiddenlayer_values)\n",
    "    ctivationlayer_1 = np.apply_along_axis(sigmoid,1,hiddenlayer_values) #hiddenlayer_activations\n",
    "    \n",
    "    finallayer_values = np.dot(activationlayer_1 , wout1)+bout1\n",
    "    #activationlayer_2 = tanh(finallayer_values)\n",
    "    activationlayer_2 = np.apply_along_axis(sigmoid,1,finallayer_values) #output \n",
    "    pred = activationlayer_2\n",
    "    \n",
    "    Error = mse(y_train , pred) #All four inputs are trained simultaneously and give a prediction. Compute error wrt these predictions.\n",
    "    Error_prime = mse_prime(y_train , pred)\n",
    "    \n",
    "    #backact2 = Error_prime * derivatives_sigmoid(finallayer_values)#???????????\n",
    "    backact2 = Error_prime * derivatives_sigmoid(pred) #d_output\n",
    "    #backact2 = Error_prime * tanh_prime(finallayer_values)\n",
    "  \n",
    "    \n",
    "    \n",
    "    backfinal = np.dot(backact2 , wout1.T)\n",
    "    #backact1 = backfinal * derivatives_sigmoid(hiddenlayer_values)#????????????? \n",
    "    #backact1 = backfinal * tanh_prime(hiddenlayer_values)\n",
    "    \n",
    "    backact1 = backfinal * derivatives_sigmoid(activationlayer_1)\n",
    "   \n",
    "    backhid = np.dot(backact1 , wh1.T)\n",
    "    \n",
    "    \n",
    "    wout1 -= activationlayer_1.T.dot(backact2) *lr\n",
    "    bout1 -= np.sum(backact2, axis=0,keepdims=True) *lr\n",
    "    wh1 -= x_train.T.dot(backact1) *lr\n",
    "    bh1 -= np.sum(backact1, axis=0,keepdims=True) *lr\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7d9b9fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual :\n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]] \n",
      "\n",
      "predicted :\n",
      " [[0.02065139]\n",
      " [0.98106628]\n",
      " [0.97970001]\n",
      " [0.02442105]]\n"
     ]
    }
   ],
   "source": [
    "print('actual :\\n', y_train, '\\n')\n",
    "print('predicted :\\n', pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0299b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55120826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f63e0ce",
   "metadata": {},
   "source": [
    "# Explaining Using an Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e737dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input array\n",
    "#X=np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
    "\n",
    "#Output\n",
    "#y=np.array([[1],[1],[0]])\n",
    "\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "#Sigmoid Function\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "#Derivative of Sigmoid Function\n",
    "def derivatives_sigmoid(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa8cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable initialization\n",
    "\n",
    "epoch=10000 #Setting training iterations\n",
    "lr=0.1 #Setting learning rate\n",
    "inputlayer_neurons = X.shape[1] #number of features in data set\n",
    "hiddenlayer_neurons = 3 #number of hidden layers neurons\n",
    "output_neurons = 1 #number of neurons at output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bce19db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight and bias initialization\n",
    "wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))  # number of weights between two layers = i x j ||\n",
    "bh=np.random.uniform(size=(1,hiddenlayer_neurons))  ## number of biases = number of output neurons / neurons in next layer\n",
    "\n",
    "wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
    "bout=np.random.uniform(size=(1,output_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdc8d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epoch):\n",
    "    \n",
    "    # Input (4 numbers) --> Hidden Layer (3 neuron) -- Activation Layer for Hidden layer (sigmoid) --> Final Layer (1 neuron)- \n",
    "    # -- Activation Layer for Final Layer (1 neuron) == Output prediction\n",
    "    \n",
    "   #Forward Propogation (Here all the inputs are taken at once)\n",
    "    hidden_layer_input1=np.dot(X,wh)  ## Because Hidden Layer values Y =  X.W + B\n",
    "    hidden_layer_input=hidden_layer_input1 + bh ## Because Hidden Layer values Y =  X.W + B\n",
    "    hiddenlayer_activations = sigmoid(hidden_layer_input) ##Apply activation\n",
    "    output_layer_input1=np.dot(hiddenlayer_activations,wout)\n",
    "    output_layer_input= output_layer_input1+ bout\n",
    "    output = sigmoid(output_layer_input)  ## Same as above for last step \n",
    "\n",
    "  #Backpropagation\n",
    "    E = output-y    #Error is simply defined as y*-y\n",
    "    \n",
    "            \n",
    "    #del(E)/del(Y) = [1] or [0]\n",
    "    #No parameters to update for Activation layer \n",
    "    slope_output_layer = derivatives_sigmoid(output) #slope = gradient in 2D\n",
    "    slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)  \n",
    "    d_output = E * slope_output_layer # del(E)/del(X) = E_prime * f'(X)\n",
    "    \n",
    "    \n",
    "    Error_at_hidden_layer = d_output.dot(wout.T) # del(E)/del(X) = del(E)/del(Y) * W^T\n",
    "    d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer  # For activation layer, del(E)/del(X) = del(E)/del(Y)*f'(X)\n",
    "    \n",
    "    \n",
    "    #Updating Weights and Biases  \n",
    "    wout -= hiddenlayer_activations.T.dot(d_output) *lr\n",
    "    bout -= np.sum(d_output, axis=0,keepdims=True) *lr\n",
    "    wh -= X.T.dot(d_hiddenlayer) *lr\n",
    "    bh -= np.sum(d_hiddenlayer, axis=0,keepdims=True) *lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009641a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('actual :\\n', y, '\\n')\n",
    "print('predicted :\\n', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7968fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('actual :\\n', y, '\\n')\n",
    "print('predicted :\\n', np.round(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ea8e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ba816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b46336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
